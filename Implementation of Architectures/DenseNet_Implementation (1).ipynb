{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "!pip install keras --user"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpeQR-YpG1Y3",
        "outputId": "cdddea8c-385b-4379-e974-6464b76a4dd6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EBDY0Iq67Cd3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.metrics import classification_report, log_loss, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LHXiCaJk7j7k"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/drive/MyDrive/asl_alphabet_train/asl_alphabet_train'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Name=os.listdir(data_dir)\n",
        "print(Name)\n",
        "print(len(Name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rC6Uv8hvGzmy",
        "outputId": "a6617441-60be-4b7a-f105-2cb5ebded2a3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'space', 'nothing']\n",
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L3QZGNc7w9z",
        "outputId": "573c2f84-cc3f-469f-a631-7df5a5c9f39e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29/29 [26:20<00:00, 54.48s/it]\n"
          ]
        }
      ],
      "source": [
        "data_image=[]\n",
        "data_label=[]\n",
        "count=0\n",
        "for name in tqdm(Name):\n",
        "    path=os.path.join(data_dir,name)\n",
        "    for im in os.listdir(path):\n",
        "        image=load_img(os.path.join(path,im), grayscale=False, color_mode='rgb', target_size=(50,50))\n",
        "        image=img_to_array(image)\n",
        "        image=image/255.0\n",
        "        data_image+=[image]\n",
        "        data_label+=[count]\n",
        "    count+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hQgBOkDr8jSa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628c4737-778d-4cc8-9934-5c9c3b8a7f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89097\n"
          ]
        }
      ],
      "source": [
        "m=len(data_image)\n",
        "M=list(range(m))\n",
        "random.seed(2021)\n",
        "random.shuffle(M)\n",
        "print(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "T-9cfG3IE5EJ"
      },
      "outputs": [],
      "source": [
        "data_image=np.array(data_image)\n",
        "data_label=np.array(data_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AaZNNJKLE80N"
      },
      "outputs": [],
      "source": [
        "trainX=data_image[M[0:(m//4)*3]]\n",
        "trainY0=data_label[M[0:(m//4)*3]]\n",
        "testX=data_image[M[(m//4)*3:]]\n",
        "testY0=data_label[M[(m//4)*3:]]\n",
        "\n",
        "N=list(range(len(Name)))    \n",
        "normal_mapping=dict(zip(Name,N)) \n",
        "reverse_mapping=dict(zip(N,Name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainY0=pd.Series(trainY0)\n",
        "testY0=pd.Series(testY0)"
      ],
      "metadata": {
        "id": "1RmjhK5ZObvd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels1=to_categorical(trainY0)\n",
        "trainY=np.array(labels1)\n",
        "trainx,testx,trainy,testy=train_test_split(trainX,trainY,test_size=0.2,random_state=44)"
      ],
      "metadata": {
        "id": "j6nULCAMOe0a"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainx.shape)\n",
        "print(testx.shape)\n",
        "print(trainy.shape)\n",
        "print(testy.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owEtcNowOhVc",
        "outputId": "064f4745-c44f-4735-d88d-03eadec504a0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(53457, 50, 50, 3)\n",
            "(13365, 50, 50, 3)\n",
            "(53457, 29)\n",
            "(13365, 29)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rotation_range=20,zoom_range=0.6,\n",
        "                        width_shift_range=0.2,height_shift_range=0.2,shear_range=0.1,fill_mode=\"nearest\")"
      ],
      "metadata": {
        "id": "sQPC6rQ2OipX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model3 = tf.keras.applications.DenseNet201(input_shape=(50,50,3),include_top=False,weights='imagenet',pooling='avg')\n",
        "pretrained_model3.trainable = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gInMfz5AOkps",
        "outputId": "a6131f88-e576-4c7e-c458-f539e5b1c8de"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "74836368/74836368 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs3 = pretrained_model3.input\n",
        "x3 = tf.keras.layers.Dense(128, activation='relu')(pretrained_model3.output)\n",
        "outputs3 = tf.keras.layers.Dense(29, activation='softmax')(x3)\n",
        "model = tf.keras.Model(inputs=inputs3, outputs=outputs3)"
      ],
      "metadata": {
        "id": "z19fVFqMOmgO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "his=model.fit(datagen.flow(trainx,trainy,batch_size=16),validation_data=(testx,testy),epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_CDOhTKOzRv",
        "outputId": "e8544d12-3d6a-4da2-c7e1-e616ea0aa81a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3342/3342 [==============================] - ETA: 0s - batch: 1670.5000 - size: 15.9955 - loss: 2.1635 - acc: 0.3605"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3342/3342 [==============================] - 172s 50ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1635 - acc: 0.3605 - val_loss: 2.8671 - val_acc: 0.2426\n",
            "Epoch 2/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1463 - acc: 0.3620 - val_loss: 2.7541 - val_acc: 0.2519\n",
            "Epoch 3/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1530 - acc: 0.3639 - val_loss: 2.6520 - val_acc: 0.2625\n",
            "Epoch 4/100\n",
            "3342/3342 [==============================] - 165s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1471 - acc: 0.3661 - val_loss: 2.6844 - val_acc: 0.2530\n",
            "Epoch 5/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1488 - acc: 0.3638 - val_loss: 2.7205 - val_acc: 0.2587\n",
            "Epoch 6/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1475 - acc: 0.3623 - val_loss: 2.6400 - val_acc: 0.2655\n",
            "Epoch 7/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1576 - acc: 0.3586 - val_loss: 2.6642 - val_acc: 0.2675\n",
            "Epoch 8/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1474 - acc: 0.3649 - val_loss: 2.7264 - val_acc: 0.2593\n",
            "Epoch 9/100\n",
            "3342/3342 [==============================] - 162s 48ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1482 - acc: 0.3633 - val_loss: 2.6397 - val_acc: 0.2666\n",
            "Epoch 10/100\n",
            "3342/3342 [==============================] - 162s 48ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1516 - acc: 0.3633 - val_loss: 2.7835 - val_acc: 0.2482\n",
            "Epoch 11/100\n",
            "3342/3342 [==============================] - 162s 48ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1355 - acc: 0.3690 - val_loss: 2.7831 - val_acc: 0.2592\n",
            "Epoch 12/100\n",
            "3342/3342 [==============================] - 162s 48ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1515 - acc: 0.3639 - val_loss: 2.8234 - val_acc: 0.2622\n",
            "Epoch 13/100\n",
            "3342/3342 [==============================] - 161s 48ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1420 - acc: 0.3651 - val_loss: 2.6687 - val_acc: 0.2648\n",
            "Epoch 14/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1454 - acc: 0.3653 - val_loss: 2.7937 - val_acc: 0.2734\n",
            "Epoch 15/100\n",
            "3342/3342 [==============================] - 162s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1311 - acc: 0.3672 - val_loss: 2.6568 - val_acc: 0.2697\n",
            "Epoch 16/100\n",
            "3342/3342 [==============================] - 162s 48ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1426 - acc: 0.3644 - val_loss: 2.7601 - val_acc: 0.2466\n",
            "Epoch 17/100\n",
            "3342/3342 [==============================] - 162s 48ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1363 - acc: 0.3660 - val_loss: 2.5929 - val_acc: 0.2673\n",
            "Epoch 18/100\n",
            "3342/3342 [==============================] - 161s 48ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1379 - acc: 0.3665 - val_loss: 2.7054 - val_acc: 0.2575\n",
            "Epoch 19/100\n",
            "3342/3342 [==============================] - 159s 47ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1336 - acc: 0.3677 - val_loss: 2.6880 - val_acc: 0.2617\n",
            "Epoch 20/100\n",
            "3342/3342 [==============================] - 161s 48ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1358 - acc: 0.3695 - val_loss: 2.6550 - val_acc: 0.2557\n",
            "Epoch 21/100\n",
            "3342/3342 [==============================] - 171s 51ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1401 - acc: 0.3674 - val_loss: 2.7785 - val_acc: 0.2570\n",
            "Epoch 22/100\n",
            "3342/3342 [==============================] - 168s 50ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1321 - acc: 0.3701 - val_loss: 2.6625 - val_acc: 0.2501\n",
            "Epoch 23/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1397 - acc: 0.3643 - val_loss: 2.6927 - val_acc: 0.2637\n",
            "Epoch 24/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1363 - acc: 0.3682 - val_loss: 2.6748 - val_acc: 0.2524\n",
            "Epoch 25/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1278 - acc: 0.3685 - val_loss: 2.6891 - val_acc: 0.2464\n",
            "Epoch 26/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1369 - acc: 0.3657 - val_loss: 2.7284 - val_acc: 0.2525\n",
            "Epoch 27/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1276 - acc: 0.3669 - val_loss: 2.8048 - val_acc: 0.2438\n",
            "Epoch 28/100\n",
            "3342/3342 [==============================] - 162s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1244 - acc: 0.3700 - val_loss: 2.8071 - val_acc: 0.2507\n",
            "Epoch 29/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1311 - acc: 0.3674 - val_loss: 2.7227 - val_acc: 0.2550\n",
            "Epoch 30/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1274 - acc: 0.3674 - val_loss: 2.7707 - val_acc: 0.2539\n",
            "Epoch 31/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1231 - acc: 0.3693 - val_loss: 2.7488 - val_acc: 0.2556\n",
            "Epoch 32/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1217 - acc: 0.3696 - val_loss: 2.7165 - val_acc: 0.2608\n",
            "Epoch 33/100\n",
            "3342/3342 [==============================] - 166s 50ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1164 - acc: 0.3724 - val_loss: 2.8215 - val_acc: 0.2582\n",
            "Epoch 34/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1319 - acc: 0.3681 - val_loss: 2.7674 - val_acc: 0.2573\n",
            "Epoch 35/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1260 - acc: 0.3692 - val_loss: 2.7113 - val_acc: 0.2560\n",
            "Epoch 36/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1191 - acc: 0.3707 - val_loss: 2.7366 - val_acc: 0.2567\n",
            "Epoch 37/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1193 - acc: 0.3731 - val_loss: 2.6980 - val_acc: 0.2430\n",
            "Epoch 38/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1307 - acc: 0.3672 - val_loss: 2.6483 - val_acc: 0.2671\n",
            "Epoch 39/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1182 - acc: 0.3723 - val_loss: 2.7551 - val_acc: 0.2577\n",
            "Epoch 40/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1196 - acc: 0.3726 - val_loss: 2.7576 - val_acc: 0.2555\n",
            "Epoch 41/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1099 - acc: 0.3719 - val_loss: 2.7319 - val_acc: 0.2703\n",
            "Epoch 42/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1164 - acc: 0.3749 - val_loss: 2.6896 - val_acc: 0.2771\n",
            "Epoch 43/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1196 - acc: 0.3722 - val_loss: 2.7012 - val_acc: 0.2709\n",
            "Epoch 44/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1162 - acc: 0.3728 - val_loss: 2.6863 - val_acc: 0.2672\n",
            "Epoch 45/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1158 - acc: 0.3698 - val_loss: 2.7722 - val_acc: 0.2594\n",
            "Epoch 46/100\n",
            "3342/3342 [==============================] - 162s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1146 - acc: 0.3728 - val_loss: 2.9780 - val_acc: 0.2459\n",
            "Epoch 47/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1152 - acc: 0.3718 - val_loss: 2.8373 - val_acc: 0.2515\n",
            "Epoch 48/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1093 - acc: 0.3723 - val_loss: 2.7801 - val_acc: 0.2595\n",
            "Epoch 49/100\n",
            "3342/3342 [==============================] - 165s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1071 - acc: 0.3726 - val_loss: 2.7971 - val_acc: 0.2613\n",
            "Epoch 50/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1120 - acc: 0.3708 - val_loss: 2.6638 - val_acc: 0.2750\n",
            "Epoch 51/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1138 - acc: 0.3751 - val_loss: 2.6378 - val_acc: 0.2758\n",
            "Epoch 52/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1147 - acc: 0.3709 - val_loss: 2.7529 - val_acc: 0.2575\n",
            "Epoch 53/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1138 - acc: 0.3743 - val_loss: 2.7295 - val_acc: 0.2696\n",
            "Epoch 54/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1046 - acc: 0.3753 - val_loss: 2.7777 - val_acc: 0.2663\n",
            "Epoch 55/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1147 - acc: 0.3719 - val_loss: 2.6463 - val_acc: 0.2829\n",
            "Epoch 56/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1020 - acc: 0.3741 - val_loss: 2.7096 - val_acc: 0.2554\n",
            "Epoch 57/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1100 - acc: 0.3732 - val_loss: 2.6765 - val_acc: 0.2623\n",
            "Epoch 58/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1160 - acc: 0.3700 - val_loss: 2.6768 - val_acc: 0.2719\n",
            "Epoch 59/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1039 - acc: 0.3748 - val_loss: 2.6446 - val_acc: 0.2670\n",
            "Epoch 60/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0984 - acc: 0.3736 - val_loss: 2.6985 - val_acc: 0.2641\n",
            "Epoch 61/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1000 - acc: 0.3762 - val_loss: 2.5021 - val_acc: 0.2827\n",
            "Epoch 62/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1057 - acc: 0.3727 - val_loss: 2.6435 - val_acc: 0.2594\n",
            "Epoch 63/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0997 - acc: 0.3743 - val_loss: 2.7431 - val_acc: 0.2631\n",
            "Epoch 64/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1060 - acc: 0.3749 - val_loss: 2.7831 - val_acc: 0.2581\n",
            "Epoch 65/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1008 - acc: 0.3726 - val_loss: 2.6149 - val_acc: 0.2629\n",
            "Epoch 66/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1036 - acc: 0.3733 - val_loss: 2.7468 - val_acc: 0.2782\n",
            "Epoch 67/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0891 - acc: 0.3777 - val_loss: 2.5575 - val_acc: 0.2938\n",
            "Epoch 68/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0988 - acc: 0.3742 - val_loss: 2.6460 - val_acc: 0.2793\n",
            "Epoch 69/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0999 - acc: 0.3746 - val_loss: 2.6768 - val_acc: 0.2631\n",
            "Epoch 70/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0922 - acc: 0.3775 - val_loss: 2.7123 - val_acc: 0.2772\n",
            "Epoch 71/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.1054 - acc: 0.3751 - val_loss: 2.7583 - val_acc: 0.2699\n",
            "Epoch 72/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0937 - acc: 0.3753 - val_loss: 2.7270 - val_acc: 0.2761\n",
            "Epoch 73/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0948 - acc: 0.3779 - val_loss: 2.7405 - val_acc: 0.2687\n",
            "Epoch 74/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0952 - acc: 0.3768 - val_loss: 2.6552 - val_acc: 0.2823\n",
            "Epoch 75/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0962 - acc: 0.3761 - val_loss: 2.7630 - val_acc: 0.2791\n",
            "Epoch 76/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0882 - acc: 0.3781 - val_loss: 2.8896 - val_acc: 0.2637\n",
            "Epoch 77/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0971 - acc: 0.3770 - val_loss: 2.8411 - val_acc: 0.2667\n",
            "Epoch 78/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0835 - acc: 0.3799 - val_loss: 2.8155 - val_acc: 0.2597\n",
            "Epoch 79/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0939 - acc: 0.3764 - val_loss: 2.7021 - val_acc: 0.2662\n",
            "Epoch 80/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0949 - acc: 0.3771 - val_loss: 2.6911 - val_acc: 0.2658\n",
            "Epoch 81/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0982 - acc: 0.3754 - val_loss: 2.6833 - val_acc: 0.2762\n",
            "Epoch 82/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0950 - acc: 0.3772 - val_loss: 2.7765 - val_acc: 0.2696\n",
            "Epoch 83/100\n",
            "3342/3342 [==============================] - 165s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0930 - acc: 0.3775 - val_loss: 2.6701 - val_acc: 0.2827\n",
            "Epoch 84/100\n",
            "3342/3342 [==============================] - 167s 50ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0990 - acc: 0.3756 - val_loss: 2.5849 - val_acc: 0.2760\n",
            "Epoch 85/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0992 - acc: 0.3789 - val_loss: 2.7998 - val_acc: 0.2593\n",
            "Epoch 86/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0930 - acc: 0.3797 - val_loss: 2.8796 - val_acc: 0.2582\n",
            "Epoch 87/100\n",
            "3342/3342 [==============================] - 163s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0909 - acc: 0.3781 - val_loss: 2.8560 - val_acc: 0.2584\n",
            "Epoch 88/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0972 - acc: 0.3769 - val_loss: 2.8231 - val_acc: 0.2522\n",
            "Epoch 89/100\n",
            "3342/3342 [==============================] - 165s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0852 - acc: 0.3780 - val_loss: 2.6835 - val_acc: 0.2826\n",
            "Epoch 90/100\n",
            "3342/3342 [==============================] - 165s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0879 - acc: 0.3781 - val_loss: 2.6807 - val_acc: 0.2705\n",
            "Epoch 91/100\n",
            "3342/3342 [==============================] - 165s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0893 - acc: 0.3778 - val_loss: 2.6013 - val_acc: 0.2753\n",
            "Epoch 92/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0823 - acc: 0.3825 - val_loss: 2.6536 - val_acc: 0.2780\n",
            "Epoch 93/100\n",
            "3342/3342 [==============================] - 165s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0870 - acc: 0.3800 - val_loss: 2.6757 - val_acc: 0.2682\n",
            "Epoch 94/100\n",
            "3342/3342 [==============================] - 165s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0867 - acc: 0.3819 - val_loss: 2.7091 - val_acc: 0.2746\n",
            "Epoch 95/100\n",
            "3342/3342 [==============================] - 165s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0716 - acc: 0.3823 - val_loss: 2.9945 - val_acc: 0.2516\n",
            "Epoch 96/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0772 - acc: 0.3791 - val_loss: 2.9162 - val_acc: 0.2498\n",
            "Epoch 97/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0962 - acc: 0.3766 - val_loss: 2.8669 - val_acc: 0.2543\n",
            "Epoch 98/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0810 - acc: 0.3814 - val_loss: 2.9548 - val_acc: 0.2575\n",
            "Epoch 99/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0840 - acc: 0.3796 - val_loss: 2.8560 - val_acc: 0.2621\n",
            "Epoch 100/100\n",
            "3342/3342 [==============================] - 164s 49ms/step - batch: 1670.5000 - size: 15.9955 - loss: 2.0852 - acc: 0.3793 - val_loss: 2.9378 - val_acc: 0.2589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model.predict(testx)\n",
        "pred=np.argmax(y_pred,axis=1)\n",
        "ground = np.argmax(testy,axis=1)\n",
        "print(classification_report(ground,pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob_eUoutO4nc",
        "outputId": "9a684c41-5e56-4b88-f331-5f5f93b1149a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.10      0.80      0.17       467\n",
            "           1       0.15      0.30      0.20       519\n",
            "           2       0.47      0.59      0.52       480\n",
            "           3       0.30      0.21      0.24       460\n",
            "           4       0.17      0.11      0.13       447\n",
            "           5       0.88      0.33      0.48       448\n",
            "           6       0.62      0.14      0.23       458\n",
            "           7       0.44      0.45      0.45       454\n",
            "           8       0.45      0.17      0.24       482\n",
            "           9       0.29      0.43      0.35       526\n",
            "          10       0.17      0.03      0.06       445\n",
            "          11       0.16      0.41      0.23       441\n",
            "          12       0.28      0.21      0.24       435\n",
            "          13       0.11      0.09      0.10       460\n",
            "          14       0.60      0.59      0.59       451\n",
            "          15       0.32      0.61      0.42       448\n",
            "          16       0.49      0.26      0.34       464\n",
            "          17       0.27      0.15      0.19       464\n",
            "          18       0.19      0.19      0.19       479\n",
            "          19       0.00      0.00      0.00       451\n",
            "          20       0.15      0.04      0.06       438\n",
            "          21       0.38      0.07      0.12       488\n",
            "          22       0.72      0.13      0.21       461\n",
            "          23       0.46      0.03      0.05       474\n",
            "          24       0.25      0.00      0.00       430\n",
            "          25       0.37      0.02      0.04       470\n",
            "          26       0.46      0.35      0.40       433\n",
            "          27       0.57      0.02      0.03       456\n",
            "          28       0.50      0.77      0.61       436\n",
            "\n",
            "    accuracy                           0.26     13365\n",
            "   macro avg       0.36      0.26      0.24     13365\n",
            "weighted avg       0.35      0.26      0.24     13365\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ePPskgcbSO3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}